<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="theme-color" content="#4285f4">
    <meta name="description" content="A simple Progressive Web App template">
    <title>Speech to Braille Refreshable Module</title>
    <link rel="stylesheet" href="css/style.css">
    <link rel="manifest" href="manifest.json">
    <link rel="apple-touch-icon" href="images/icons/icon-192x192.png">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
            margin: 0;
            padding: 0;
            color: #333;
            background-color: #f5f5f5;
        }
        
        header {
            background-color: #4285f4;
            color: white;
            padding: 1rem;
            text-align: center;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }
        
        main {
            max-width: 800px;
            margin: 0 auto;
            padding: 1rem;
        }
        
        .card {
            background-color: white;
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 1rem;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }
        
        .status {
            padding: 1rem;
            border-radius: 8px;
            margin-bottom: 1rem;
            background-color: #e0e0e0;
        }
        
        .online {
            background-color: #d4edda;
            color: #155724;
        }
        
        .offline {
            background-color: #f8d7da;
            color: #721c24;
        }
        
        button {
            background-color: #4285f4;
            color: white;
            border: none;
            padding: 0.75rem 1.5rem;
            border-radius: 4px;
            cursor: pointer;
            font-size: 1rem;
            transition: background-color 0.3s;
        }
        
        button:hover {
            background-color: #3367d6;
        }
        
        .install-button {
            display: none;
            margin: 1rem 0;
        }
        
        footer {
            text-align: center;
            padding: 1rem;
            color: #666;
            font-size: 0.875rem;
        }
        
        /* Speech Recognition Styles */
        #speech-controls {
            display: flex;
            align-items: center;
            gap: 1rem;
            margin-bottom: 1rem;
        }
        
        #recording-indicator {
            padding: 0.5rem 1rem;
            border-radius: 4px;
            font-weight: bold;
            transition: all 0.3s;
        }
        
        .recording-off {
            color: #666;
            background-color: #e0e0e0;
        }
        
        .recording-on {
            color: white;
            background-color: #dc3545;
            animation: pulse 1.5s infinite;
        }
        
        @keyframes pulse {
            0% { opacity: 1; }
            50% { opacity: 0.5; }
            100% { opacity: 1; }
        }
        
        #speech-output {
            min-height: 100px;
            margin-top: 1rem;
            padding: 1rem;
            border-radius: 4px;
            border: 1px solid #ddd;
            background-color: #f9f9f9;
        }
        
        #interim-text {
            color: #666;
            font-style: italic;
        }
        
        #final-text {
            font-weight: 500;
            margin-top: 0.5rem;
        }
        
        /* Model Status Indicator Styles */
        #model-status {
            padding: 0.5rem;
            margin-top: 1rem;
            border-radius: 4px;
            background-color: #f0f0f0;
            font-size: 0.9rem;
            display: flex;
            align-items: center;
        }
        
        .model-badge {
            padding: 0.25rem 0.5rem;
            border-radius: 3px;
            margin-left: 0.5rem;
            font-weight: 500;
            color: white;
        }
        
        .web-speech {
            background-color: #4285f4;
        }
        
        .vosk-model {
            background-color: #34a853;
        }
        
        .no-model {
            background-color: #ea4335;
        }
    </style>
</head>
<body>
    <header>
        <h1>Speech to Braille Refreshable Module</h1>
    </header>
    
    <main>
        <div id="connection-status" class="status">
            Checking connection...
        </div>
        
        <button id="install-button" class="install-button">Install App</button>
        
        <div class="card">
            <h2>Welcome to your PWA</h2>
            <p>This Progressive Web App template is designed to work offline on mobile devices. It includes:</p>
            <ul>
                <li>Service worker for offline caching</li>
                <li>Web App Manifest for installation</li>
                <li>Responsive design</li>
                <li>Network status detection</li>
            </ul>
        </div>
        
        <div class="card">
            <h2>Cached Content</h2>
            <p>This content will be available even when you're offline.</p>
            <p>Last updated: <span id="last-updated">Loading...</span></p>
        </div>
        
        <div class="card">
            <h2>Speech Recognition</h2>
            <p>Speak into your microphone and see the text appear in real-time.</p>
            <div id="speech-controls">
                <button id="start-speech-btn">Start Speaking</button>
                <button id="stop-speech-btn" disabled>Stop</button>
                <button id="load-model-btn">Load Speech Model</button>
                <div id="recording-indicator" class="recording-off">● Recording</div>
            </div>
            <div id="model-status">
                Recognition Model: <span class="model-badge no-model" id="model-badge">Not Selected</span>
            </div>
            <div id="speech-output">
                <p id="interim-text"></p>
                <p id="final-text"></p>
            </div>
        </div>
        
        <div class="card">
            <h2>Dynamic Content</h2>
            <p>This section demonstrates dynamic content handling:</p>
            <button id="refresh-button">Refresh Data</button>
            <div id="dynamic-content">
                <p>Content will appear here...</p>
            </div>
        </div>
    </main>
    
    <footer>
        <p>Progressive Web App Template &copy; 2025</p>
    </footer>

    <script src="node_modules/vosk-browser/dist/vosk.js"></script>
    <script>
        // Speech recognition functionality
        let recognitionActive = false;
        let audioStream = null;
        let recognizer = null;
        let audioContext = null;
        let processor = null;
        let source = null;
        let modelLoaded = false;
        
        // DOM elements
        const startSpeechBtn = document.getElementById('start-speech-btn');
        const stopSpeechBtn = document.getElementById('stop-speech-btn');
        const loadModelBtn = document.getElementById('load-model-btn');
        const recordingIndicator = document.getElementById('recording-indicator');
        const interimTextElement = document.getElementById('interim-text');
        const finalTextElement = document.getElementById('final-text');
        const modelBadge = document.getElementById('model-badge');
        
        // Remove any old buttons that were added programmatically
        const oldButtons = document.querySelectorAll('main > button');
        oldButtons.forEach(button => {
            if (button.textContent === 'Start Speech Recognition' || 
                button.textContent === 'Load Speech Recognition Model') {
                button.remove();
            }
        });
        
        // Load model button setup
        loadModelBtn.addEventListener('click', async () => {
            loadModelBtn.disabled = true;
            loadModelBtn.textContent = 'Loading Model...';
            try {
                await initializeVosk();
                loadModelBtn.textContent = 'Model Loaded';
                loadModelBtn.style.backgroundColor = '#4CAF50'; // Green color for success
                modelLoaded = true;
                updateModelStatus('vosk');
            } catch (error) {
                console.error('Error loading Vosk model:', error);
                loadModelBtn.textContent = 'Load Model';
                loadModelBtn.disabled = false;
                updateModelStatus('none');
                alert('Failed to load the speech recognition model. Please check your connection.');
            }
        });

        // Initialize Vosk model
        async function initializeVosk() {
            try {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const model = new vosk.Model('https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip');
                recognizer = new vosk.Recognizer({
                    model: model,
                    sampleRate: audioContext.sampleRate
                });
                console.log('Vosk model loaded successfully');
                return recognizer;
            } catch (error) {
                console.error('Error initializing Vosk:', error);
                throw error;
            }
        }
        
        // Update model status indicator
        function updateModelStatus(model) {
            modelBadge.className = 'model-badge';
            
            switch(model) {
                case 'webspeech':
                    modelBadge.classList.add('web-speech');
                    modelBadge.textContent = 'Web Speech API';
                    break;
                case 'vosk':
                    modelBadge.classList.add('vosk-model');
                    modelBadge.textContent = 'Vosk Model';
                    break;
                default:
                    modelBadge.classList.add('no-model');
                    modelBadge.textContent = 'Not Selected';
            }
        }
        
        // Speech recognition initialization
        async function initializeSpeechRecognition() {
            // First try to use Web Speech API if available (better for real-time results)
            if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
                console.log('Using Web Speech API for speech recognition');
                updateModelStatus('webspeech');
                
                // Create speech recognition object
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                const recognition = new SpeechRecognition();
                
                // Configure recognition
                recognition.continuous = true;
                recognition.interimResults = true;
                recognition.lang = 'en-US';
                
                // Set up event handlers
                recognition.onstart = () => {
                    setRecordingState(true);
                };
                
                recognition.onend = () => {
                    setRecordingState(false);
                };
                
                recognition.onresult = (event) => {
                    let interimTranscript = '';
                    let finalTranscript = '';
                    
                    // Process results
                    for (let i = event.resultIndex; i < event.results.length; ++i) {
                        if (event.results[i].isFinal) {
                            finalTranscript += event.results[i][0].transcript + ' ';
                        } else {
                            interimTranscript += event.results[i][0].transcript;
                        }
                    }
                    
                    // Update the UI with results
                    if (interimTranscript) {
                        interimTextElement.textContent = interimTranscript;
                    }
                    
                    if (finalTranscript) {
                        // Append to final text, don't replace it
                        finalTextElement.textContent += finalTranscript;
                        // Clear interim text once it's finalized
                        interimTextElement.textContent = '';
                    }
                };
                
                recognition.onerror = (event) => {
                    console.error('Speech recognition error:', event.error);
                    setRecordingState(false);
                    alert(`Speech recognition error: ${event.error}`);
                };
                
                // Function to start recognition
                startSpeechBtn.addEventListener('click', () => {
                    if (!recognitionActive) {
                        try {
                            recognition.start();
                        } catch (error) {
                            console.error('Failed to start recognition:', error);
                        }
                    }
                });
                
                // Function to stop recognition
                stopSpeechBtn.addEventListener('click', () => {
                    if (recognitionActive) {
                        recognition.stop();
                    }
                });
                
            } else {
                // Fall back to Vosk for browsers without Web Speech API or offline use
                console.log('Web Speech API not available, using Vosk');
                updateModelStatus('none'); // Start with no model until loaded
                
                // Set up start speech button
                startSpeechBtn.addEventListener('click', () => {
                    if (modelLoaded) {
                        startVoskRecognition();
                    } else {
                        alert('Please load the speech model first by clicking the "Load Speech Model" button.');
                    }
                });
                
                // Set up stop speech button
                stopSpeechBtn.addEventListener('click', stopVoskRecognition);
            }
        }
        
        // Start Vosk recognition
        async function startVoskRecognition() {
            if (recognitionActive || !recognizer) return;
            
            try {
                audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                source = audioContext.createMediaStreamSource(audioStream);
                processor = audioContext.createScriptProcessor(4096, 1, 1);
                
                source.connect(processor);
                processor.connect(audioContext.destination);
                
                processor.onaudioprocess = (event) => {
                    // Convert audio data for Vosk
                    const inputData = event.inputBuffer.getChannelData(0);
                    const int16Array = new Int16Array(inputData.length);
                    for (let i = 0; i < inputData.length; i++) {
                        int16Array[i] = inputData[i] * 32767;
                    }
                    
                    // Process with Vosk
                    const result = recognizer.acceptWaveform(int16Array);
                    
                    if (result) {
                        // Final result
                        const finalText = recognizer.result().text;
                        if (finalText) {
                            finalTextElement.textContent += finalText + ' ';
                        }
                    } else {
                        // Partial result (intermediate)
                        const partialText = recognizer.partialResult().partial;
                        if (partialText) {
                            interimTextElement.textContent = partialText;
                        }
                    }
                };
                
                setRecordingState(true);
                updateModelStatus('vosk'); // Ensure model status is updated when using Vosk
            } catch (error) {
                console.error('Error starting Vosk recognition:', error);
                alert('Failed to access the microphone. Please ensure you have granted permission.');
                setRecordingState(false);
            }
        }
        
        // Stop Vosk recognition
        function stopVoskRecognition() {
            if (!recognitionActive) return;
            
            // Clean up resources
            if (source) {
                source.disconnect();
                source = null;
            }
            
            if (processor) {
                processor.disconnect();
                processor = null;
            }
            
            if (audioStream) {
                audioStream.getTracks().forEach(track => track.stop());
                audioStream = null;
            }
            
            // Reset UI
            setRecordingState(false);
        }
        
        // Helper function to update UI for recording state
        function setRecordingState(isRecording) {
            recognitionActive = isRecording;
            
            // Update button states
            startSpeechBtn.disabled = isRecording;
            stopSpeechBtn.disabled = !isRecording;
            
            // Update recording indicator
            if (isRecording) {
                recordingIndicator.className = 'recording-on';
                recordingIndicator.textContent = '● Recording';
            } else {
                recordingIndicator.className = 'recording-off';
                recordingIndicator.textContent = '● Recording';
            }
            
            // Clear interim text if stopping
            if (!isRecording) {
                interimTextElement.textContent = '';
            }
        }
        
        // Initialize speech recognition on page load
        document.addEventListener('DOMContentLoaded', initializeSpeechRecognition);
    </script>

    <script>
        // Initialize variables
        let deferredPrompt;
        const installButton = document.getElementById('install-button');
        const statusDiv = document.getElementById('connection-status');
        const lastUpdatedSpan = document.getElementById('last-updated');
        const dynamicContent = document.getElementById('dynamic-content');
        const refreshButton = document.getElementById('refresh-button');
        
        // Set the last updated time
        lastUpdatedSpan.textContent = new Date().toLocaleString();
        
        // Check if the app is online and update the UI
        function updateOnlineStatus() {
            const isOnline = navigator.onLine;
            statusDiv.textContent = isOnline ? 'You are online.' : 'You are offline. Using cached content.';
            statusDiv.className = isOnline ? 'status online' : 'status offline';
        }
        
        // Add event listeners for online/offline events
        window.addEventListener('online', updateOnlineStatus);
        window.addEventListener('offline', updateOnlineStatus);
        updateOnlineStatus(); // Initial check
        
        // Service Worker Registration
        if ('serviceWorker' in navigator) {
            window.addEventListener('load', () => {
                navigator.serviceWorker.register('service-worker.js')
                    .then(reg => {
                        console.log('Service worker registered!', reg);
                    })
                    .catch(err => {
                        console.log('Service worker registration failed:', err);
                    });
            });
        }
        
        // Handle app installation
        window.addEventListener('beforeinstallprompt', (e) => {
            // Prevent Chrome 67 and earlier from automatically showing the prompt
            e.preventDefault();
            // Stash the event so it can be triggered later
            deferredPrompt = e;
            // Show the install button
            installButton.style.display = 'block';
        });
        
        installButton.addEventListener('click', () => {
            if (deferredPrompt) {
                // Show the install prompt
                deferredPrompt.prompt();
                // Wait for the user to respond to the prompt
                deferredPrompt.userChoice.then((choiceResult) => {
                    if (choiceResult.outcome === 'accepted') {
                        console.log('User accepted the install prompt');
                        installButton.style.display = 'none';
                    } else {
                        console.log('User dismissed the install prompt');
                    }
                    deferredPrompt = null;
                });
            }
        });
        
        // Simulate fetching dynamic content
        function fetchData() {
            // In a real app, you would fetch from an API
            // Here we'll simulate with local data
            const data = {
                items: [
                    { id: 1, title: 'Item 1', description: 'Description for item 1' },
                    { id: 2, title: 'Item 2', description: 'Description for item 2' },
                    { id: 3, title: 'Item 3', description: 'Description for item 3' }
                ],
                timestamp: new Date().toLocaleString()
            };
            
            return new Promise((resolve) => {
                // Simulate network delay
                setTimeout(() => {
                    if (navigator.onLine) {
                        resolve(data);
                    } else {
                        // Return cached data if offline
                        resolve({
                            items: [{ id: 0, title: 'Cached Item', description: 'This is cached content for offline use' }],
                            timestamp: 'Using cached data'
                        });
                    }
                }, 500);
            });
        }
        
        // Update the UI with fetched data
        function updateUI(data) {
            let html = `<p>Last fetched: ${data.timestamp}</p><ul>`;
            
            data.items.forEach(item => {
                html += `<li>
                    <h3>${item.title}</h3>
                    <p>${item.description}</p>
                </li>`;
            });
            
            html += '</ul>';
            dynamicContent.innerHTML = html;
        }
        
        // Initial data fetch
        fetchData().then(updateUI);
        
        // Refresh button handler
        refreshButton.addEventListener('click', () => {
            dynamicContent.innerHTML = '<p>Loading...</p>';
            fetchData().then(updateUI);
        });
    </script>
</body>
</html>
