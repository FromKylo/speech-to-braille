<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="theme-color" content="#4285f4">
    <meta name="description" content="A simple Progressive Web App template">
    <title>Speech to Braille Refreshable Module</title>
    <link rel="stylesheet" href="css/style.css">
    <link rel="manifest" href="manifest.json">
    <link rel="apple-touch-icon" href="images/icons/icon-192x192.png">
</head>
<body>
    <header>
        <h1>Speech to Braille Refreshable Module</h1>
    </header>
    
    <main>
        <div id="connection-status" class="status">
            Checking connection...
        </div>
        
        <button id="install-button" class="install-button">Install App</button>
        
        <div class="card">
            <h2>Welcome to your PWA</h2>
            <p>This Progressive Web App template is designed to work offline on mobile devices. It includes:</p>
            <ul>
                <li>Service worker for offline caching</li>
                <li>Web App Manifest for installation</li>
                <li>Responsive design</li>
                <li>Network status detection</li>
            </ul>
        </div>
        
        <div class="card">
            <h2>Cached Content</h2>
            <p>This content will be available even when you're offline.</p>
            <p>Last updated: <span id="last-updated">Loading...</span></p>
        </div>
        
        <div class="card">
            <h2>Speech Recognition</h2>
            <p>Speak into your microphone and see the text appear in real-time.</p>
            <div id="speech-controls">
                <button id="start-speech-btn">Start Speaking</button>
                <button id="stop-speech-btn" disabled>Stop</button>
                <button id="load-model-btn">Load Speech Model</button>
                <div id="recording-indicator" class="recording-off">‚óè Recording</div>
            </div>
            <div id="model-status">
                Recognition Model: <span class="model-badge no-model" id="model-badge">Not Selected</span>
            </div>
            <div id="speech-output">
                <p id="interim-text"></p>
                <p id="final-text"></p>
            </div>
        </div>
        
        <div class="card">
            <h2>Dynamic Content</h2>
            <p>This section demonstrates dynamic content handling:</p>
            <button id="refresh-button">Refresh Data</button>
            <div id="dynamic-content">
                <p>Content will appear here...</p>
            </div>
        </div>
    </main>
    
    <footer>
        <p>Progressive Web App Template &copy; 2025</p>
    </footer>

    <script src="node_modules/vosk-browser/dist/vosk.js"></script>
    <script>
        // Speech recognition functionality
        let recognitionActive = false;
        let audioStream = null;
        let recognizer = null;
        let audioContext = null;
        let processor = null;
        let source = null;
        let modelLoaded = false;
        let voskWorker = null;
        
        // DOM elements
        const startSpeechBtn = document.getElementById('start-speech-btn');
        const stopSpeechBtn = document.getElementById('stop-speech-btn');
        const loadModelBtn = document.getElementById('load-model-btn');
        const recordingIndicator = document.getElementById('recording-indicator');
        const interimTextElement = document.getElementById('interim-text');
        const finalTextElement = document.getElementById('final-text');
        const modelBadge = document.getElementById('model-badge');
        
        // Remove any old buttons that were added programmatically
        const oldButtons = document.querySelectorAll('main > button');
        oldButtons.forEach(button => {
            if (button.textContent === 'Start Speech Recognition' || 
                button.textContent === 'Load Speech Recognition Model') {
                button.remove();
            }
        });
        
        // Load model button setup
        loadModelBtn.addEventListener('click', async () => {
            loadModelBtn.disabled = true;
            loadModelBtn.textContent = 'Loading Model...';
            try {
                await initializeVosk();
                loadModelBtn.textContent = 'Model Loaded';
                loadModelBtn.style.backgroundColor = '#4CAF50'; // Green color for success
                modelLoaded = true;
                updateModelStatus('vosk');
            } catch (error) {
                console.error('Error loading Vosk model:', error);
                loadModelBtn.textContent = 'Load Model';
                loadModelBtn.disabled = false;
                updateModelStatus('none');
                alert('Failed to load the speech recognition model. Please check your connection.');
            }
        });

        // Initialize Vosk model
        async function initializeVosk() {
            try {
                // Create audio context for sample rate
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const sampleRate = audioContext.sampleRate;
                
                // Create and initialize Web Worker for Vosk processing
                if (voskWorker) {
                    voskWorker.terminate();
                }
                
                voskWorker = new Worker('js/vosk-worker.js');
                
                // Set up worker message handler
                voskWorker.onmessage = function(event) {
                    const message = event.data;
                    switch(message.status) {
                        case 'workerReady':
                            console.log('Vosk worker is ready');
                            break;
                            
                        case 'loading':
                            loadModelBtn.textContent = 'Downloading...';
                            break;
                            
                        case 'init':
                            loadModelBtn.textContent = 'Initializing...';
                            break;
                            
                        case 'ready':
                            console.log('Vosk model loaded successfully');
                            modelLoaded = true;
                            break;
                            
                        case 'error':
                            console.error('Vosk error:', message.message);
                            loadModelBtn.textContent = 'Load Model';
                            loadModelBtn.disabled = false;
                            throw new Error(message.message);
                            
                        case 'result':
                            if (message.type === 'partial') {
                                interimTextElement.textContent = message.text;
                            } else {
                                finalTextElement.textContent += message.text + ' ';
                                interimTextElement.textContent = '';
                            }
                            break;
                    }
                };
                
                // Load the model in the worker
                voskWorker.postMessage({ 
                    command: 'loadModel',
                    sampleRate: sampleRate,
                    modelUrl: 'https://cdn.jsdelivr.net/gh/ccoreilly/vosk-browser@master/public/models/vosk-model-small-en-us-0.15'
                });
                
                // Wait for model to be ready
                return new Promise((resolve, reject) => {
                    const timeout = setTimeout(() => {
                        reject(new Error('Model loading timeout after 30 seconds'));
                    }, 30000);
                    
                    voskWorker.addEventListener('message', function handler(event) {
                        if (event.data.status === 'ready') {
                            clearTimeout(timeout);
                            voskWorker.removeEventListener('message', handler);
                            resolve(true);
                        } else if (event.data.status === 'error') {
                            clearTimeout(timeout);
                            voskWorker.removeEventListener('message', handler);
                            reject(new Error(event.data.message));
                        }
                    });
                });
            } catch (error) {
                console.error('Error initializing Vosk:', error);
                throw error;
            }
        }
        
        // Update model status indicator  
        function updateModelStatus(model) {
            modelBadge.className = 'model-badge';
            
            switch(model) {
                case 'webspeech':
                    modelBadge.classList.add('web-speech');
                    modelBadge.textContent = 'Web Speech API';
                    break;
                case 'vosk':
                    modelBadge.classList.add('vosk-model');
                    modelBadge.textContent = 'Vosk Model';
                    break;
                default:
                    modelBadge.classList.add('no-model');
                    modelBadge.textContent = 'Not Selected';
            }
        }
        
        // Speech recognition initialization
        async function initializeSpeechRecognition() {
            // First try to use Web Speech API if available (better for real-time results)
            if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
                console.log('Using Web Speech API for speech recognition');
                updateModelStatus('webspeech');
                
                // Create speech recognition object
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                const recognition = new SpeechRecognition();
                
                // Configure recognition
                recognition.continuous = true;
                recognition.interimResults = true;
                recognition.lang = 'en-US';
                
                // Set up event handlers
                recognition.onstart = () => {
                    setRecordingState(true);
                };
                
                recognition.onend = () => {
                    setRecordingState(false);
                };
                
                recognition.onresult = (event) => {
                    let interimTranscript = '';
                    let finalTranscript = '';
                    
                    // Process results
                    for (let i = event.resultIndex; i < event.results.length; ++i) {
                        if (event.results[i].isFinal) {
                            finalTranscript += event.results[i][0].transcript + ' ';
                        } else {
                            interimTranscript += event.results[i][0].transcript;
                        }
                    }
                    
                    // Update the UI with results
                    if (interimTranscript) {
                        interimTextElement.textContent = interimTranscript;
                    }
                    
                    if (finalTranscript) {
                        // Append to final text, don't replace it
                        finalTextElement.textContent += finalTranscript;
                        // Clear interim text once it's finalized
                        interimTextElement.textContent = '';
                    }
                };
                
                recognition.onerror = (event) => {
                    console.error('Speech recognition error:', event.error);
                    setRecordingState(false);
                    alert(`Speech recognition error: ${event.error}`);
                };
                
                // Function to start recognition
                startSpeechBtn.addEventListener('click', () => {
                    if (!recognitionActive) {
                        try {
                            recognition.start();
                        } catch (error) {
                            console.error('Failed to start recognition:', error);
                        }
                    }
                });
                
                // Function to stop recognition
                stopSpeechBtn.addEventListener('click', () => {
                    if (recognitionActive) {
                        recognition.stop();
                    }
                });
                
            } else {
                // Fall back to Vosk for browsers without Web Speech API or offline use
                console.log('Web Speech API not available, using Vosk');
                updateModelStatus('none'); // Start with no model until loaded
                
                // Set up start speech button
                startSpeechBtn.addEventListener('click', () => {
                    if (modelLoaded) {
                        startVoskRecognition();
                    } else {
                        alert('Please load the speech model first by clicking the "Load Speech Model" button.');
                    }
                });
                
                // Set up stop speech button
                stopSpeechBtn.addEventListener('click', stopVoskRecognition);
            }
        }
        
        // Start Vosk recognition
        async function startVoskRecognition() {
            if (recognitionActive || !voskWorker) return;
            
            try {
                audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                source = audioContext.createMediaStreamSource(audioStream);
                
                // Use ScriptProcessorNode (will be replaced with AudioWorkletNode in future version)
                processor = audioContext.createScriptProcessor(4096, 1, 1);
                
                source.connect(processor);
                processor.connect(audioContext.destination);
                
                // Reset the recognizer state
                voskWorker.postMessage({ command: 'reset' });
                
                processor.onaudioprocess = (event) => {
                    // Only process if recognition is active
                    if (recognitionActive) {
                        // Convert audio data for Vosk
                        const inputData = event.inputBuffer.getChannelData(0);
                        const int16Array = new Int16Array(inputData.length);
                        for (let i = 0; i < inputData.length; i++) {
                            int16Array[i] = Math.min(1, Math.max(-1, inputData[i])) * 32767;
                        }
                        
                        // Send to worker for processing
                        voskWorker.postMessage({
                            command: 'processAudio',
                            audio: int16Array
                        });
                    }
                };
                
                setRecordingState(true);
                
            } catch (error) {
                console.error('Error starting Vosk recognition:', error);
                alert('Failed to access the microphone. Please ensure you have granted permission.');
                setRecordingState(false);
            }
        }
        
        // Stop Vosk recognition
        function stopVoskRecognition() {
            if (!recognitionActive) return;
            
            // Clean up resources
            if (source) {
                source.disconnect();
                source = null;
            }
            
            if (processor) {
                processor.disconnect();
                processor = null;
            }
            
            if (audioContext && audioContext.state !== 'closed') {
                audioContext.suspend();
            }
            
            if (audioStream) {
                audioStream.getTracks().forEach(track => track.stop());
                audioStream = null;
            }
            
            // Reset UI
            setRecordingState(false);
        }
        
        // Helper function to update UI for recording state
        function setRecordingState(isRecording) {
            recognitionActive = isRecording;
            
            // Update button states
            startSpeechBtn.disabled = isRecording;
            stopSpeechBtn.disabled = !isRecording;
            
            // Update recording indicator
            if (isRecording) {
                recordingIndicator.className = 'recording-on';
                recordingIndicator.textContent = '‚óè Recording';
            } else {
                recordingIndicator.className = 'recording-off';
                recordingIndicator.textContent = '‚óè Recording';
            }
            
            // Clear interim text if stopping
            if (!isRecording) {
                interimTextElement.textContent = '';
            }
        }
        
        // Initialize speech recognition on page load
        document.addEventListener('DOMContentLoaded', initializeSpeechRecognition);
    </script>
    <script>
        // Initialize variables
        let deferredPrompt;
        const installButton = document.getElementById('install-button');
        const statusDiv = document.getElementById('connection-status');
        const lastUpdatedSpan = document.getElementById('last-updated');
        const dynamicContent = document.getElementById('dynamic-content');
        const refreshButton = document.getElementById('refresh-button');
        
        // Set the last updated time with local data
        lastUpdatedSpan.textContent = new Date().toLocaleString();
        
        // Check if the app is online and update the UI
        function updateOnlineStatus() {
            const isOnline = navigator.onLine;
            statusDiv.textContent = isOnline ? 'You are online.' : 'You are offline. Using cached content.';
            statusDiv.className = isOnline ? 'status online' : 'status offline';
        }
        
        // Add event listeners for online/offline events
        window.addEventListener('online', updateOnlineStatus);
        window.addEventListener('offline', updateOnlineStatus);
        updateOnlineStatus(); // Initial check
        
        // Service Worker Registration
        if ('serviceWorker' in navigator) {
            window.addEventListener('load', () => {
                navigator.serviceWorker.register('service-worker.js')
                    .then(reg => {
                        console.log('Service worker registered!', reg);
                    })
                    .catch(err => {
                        console.log('Service worker registration failed:', err);
                    });
            });
        }
        
        // Handle app installation
        window.addEventListener('beforeinstallprompt', (e) => {
            // Prevent Chrome 67 and earlier from automatically showing the prompt
            e.preventDefault();
            // Stash the event so it can be triggered later
            deferredPrompt = e;
            // Show the install button
            installButton.style.display = 'block';
        });
        
        installButton.addEventListener('click', () => {
            if (deferredPrompt) {
                // Show the install prompt
                deferredPrompt.prompt();
                // Wait for the user to respond to the prompt
                deferredPrompt.userChoice.then((choiceResult) => {
                    if (choiceResult.outcome === 'accepted') {
                        console.log('User accepted the install prompt');
                    } else {
                        console.log('User dismissed the install prompt');
                    }
                    deferredPrompt = null;
                });
            }
        });
        
        // Simulate fetching dynamic content
        function fetchData() {
            // Here we'll simulate with local data
            // In a real app, you would fetch from an API
            return new Promise((resolve) => {
                setTimeout(() => {
                    if (navigator.onLine) {
                        const data = {
                            timestamp: new Date().toLocaleString(),
                            items: [
                                { id: 1, title: 'Item 1', description: 'Description for item 1' },
                                { id: 2, title: 'Item 2', description: 'Description for item 2' },
                                { id: 3, title: 'Item 3', description: 'Description for item 3' }
                            ]
                        };
                        resolve(data);
                    } else {
                        resolve({
                            timestamp: 'Using cached data',
                            items: [{ id: 0, title: 'Cached Item', description: 'This is cached content for offline use' }]
                        });
                    }
                }, 500);
            });
        }
        
        function updateUI(data) {
            // Update the UI with fetched data
            let html = `<p>Last fetched: ${data.timestamp}</p><ul>`;
            data.items.forEach(item => {
                html += `<li>
                    <h3>${item.title}</h3>
                    <p>${item.description}</p>
                </li>`;
            });
            html += '</ul>';
            dynamicContent.innerHTML = html;
        }
        
        // Initial data fetch
        fetchData().then(updateUI);
        
        // Refresh button handler
        refreshButton.addEventListener('click', () => {
            dynamicContent.innerHTML = '<p>Loading...</p>';
            fetchData().then(updateUI);
        });
    </script>
</body>
</html>
