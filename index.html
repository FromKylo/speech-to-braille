<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="theme-color" content="#4285f4">
    <meta name="description" content="A simple Progressive Web App template">
    <title>Speech to Braille Refreshable Module</title>
    <link rel="stylesheet" href="css/style.css">
    <link rel="manifest" href="manifest.json">
    <link rel="apple-touch-icon" href="images/icons/icon-192x192.png">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
            margin: 0;
            padding: 0;
            color: #333;
            background-color: #f5f5f5;
        }
        
        header {
            background-color: #4285f4;
            color: white;
            padding: 1rem;
            text-align: center;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }
        
        main {
            max-width: 800px;
            margin: 0 auto;
            padding: 1rem;
        }
        
        .card {
            background-color: white;
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 1rem;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }
        
        .status {
            padding: 1rem;
            border-radius: 8px;
            margin-bottom: 1rem;
            background-color: #e0e0e0;
        }
        
        .online {
            background-color: #d4edda;
            color: #155724;
        }
        
        .offline {
            background-color: #f8d7da;
            color: #721c24;
        }
        
        button {
            background-color: #4285f4;
            color: white;
            border: none;
            padding: 0.75rem 1.5rem;
            border-radius: 4px;
            cursor: pointer;
            font-size: 1rem;
            transition: background-color 0.3s;
        }
        
        button:hover {
            background-color: #3367d6;
        }
        
        .install-button {
            display: none;
            margin: 1rem 0;
        }
        
        footer {
            text-align: center;
            padding: 1rem;
            color: #666;
            font-size: 0.875rem;
        }
        
        /* Speech Recognition Styles */
        #speech-controls {
            display: flex;
            align-items: center;
            gap: 1rem;
            margin-bottom: 1rem;
        }
        
        #recording-indicator {
            padding: 0.5rem 1rem;
            border-radius: 4px;
            font-weight: bold;
            transition: all 0.3s;
        }
        
        .recording-off {
            color: #666;
            background-color: #e0e0e0;
        }
        
        .recording-on {
            color: white;
            background-color: #dc3545;
            animation: pulse 1.5s infinite;
        }
        
        @keyframes pulse {
            0% { opacity: 1; }
            50% { opacity: 0.5; }
            100% { opacity: 1; }
        }
        
        #speech-output {
            min-height: 100px;
            margin-top: 1rem;
            padding: 1rem;
            border-radius: 4px;
            border: 1px solid #ddd;
            background-color: #f9f9f9;
        }
        
        #interim-text {
            color: #666;
            font-style: italic;
        }
        
        #final-text {
            font-weight: 500;
            margin-top: 0.5rem;
        }
        
        /* Model Status Indicator Styles */
        #model-status {
            padding: 0.5rem;
            margin-top: 1rem;
            border-radius: 4px;
            background-color: #f0f0f0;
            font-size: 0.9rem;
            display: flex;
            align-items: center;
        }
        
        .model-badge {
            padding: 0.25rem 0.5rem;
            border-radius: 3px;
            margin-left: 0.5rem;
            font-weight: 500;
            color: white;
        }
        
        .web-speech {
            background-color: #4285f4;
        }
        
        .vosk-model {
            background-color: #34a853;
        }
        
        .no-model {
            background-color: #ea4335;
        }
    </style>
</head>
<body>
    <header>
        <h1>Speech to Braille Refreshable Module</h1>
    </header>
    
    <main>
        <div id="connection-status" class="status">
            Checking connection...
        </div>
        
        <button id="install-button" class="install-button">Install App</button>
        
        <div class="card">
            <h2>Welcome to your PWA</h2>
            <p>This Progressive Web App template is designed to work offline on mobile devices. It includes:</p>
            <ul>
                <li>Service worker for offline caching</li>
                <li>Web App Manifest for installation</li>
                <li>Responsive design</li>
                <li>Network status detection</li>
            </ul>
        </div>
        
        <div class="card">
            <h2>Cached Content</h2>
            <p>This content will be available even when you're offline.</p>
            <p>Last updated: <span id="last-updated">Loading...</span></p>
        </div>
        
        <div class="card">
            <h2>Speech Recognition</h2>
            <p>Speak into your microphone and see the text appear in real-time.</p>
            <div id="speech-controls">
                <button id="start-speech-btn">Start Speaking</button>
                <button id="stop-speech-btn" disabled>Stop</button>
                <button id="load-model-btn">Load Speech Model</button>
                <div id="recording-indicator" class="recording-off">● Recording</div>
            </div>
            <div id="model-status">
                Recognition Model: <span class="model-badge no-model" id="model-badge">Not Selected</span>
            </div>
            <div id="speech-output">
                <p id="interim-text"></p>
                <p id="final-text"></p>
            </div>
        </div>
        
        <div class="card">
            <h2>Dynamic Content</h2>
            <p>This section demonstrates dynamic content handling:</p>
            <button id="refresh-button">Refresh Data</button>
            <div id="dynamic-content">
                <p>Content will appear here...</p>
            </div>
        </div>
    </main>
    
    <footer>
        <p>Progressive Web App Template &copy; 2025</p>
    </footer>

    <script src="node_modules/vosk-browser/dist/vosk.js"></script>
    <script>
        // Speech recognition functionality
        let recognitionActive = false;
        let audioStream = null;
        let recognizer = null;
        let audioContext = null;
        let processor = null;
        let source = null;
        let modelLoaded = false;
        let voskWorker = null;
        
        // DOM elements
        const startSpeechBtn = document.getElementById('start-speech-btn');
        const stopSpeechBtn = document.getElementById('stop-speech-btn');
        const loadModelBtn = document.getElementById('load-model-btn');
        const recordingIndicator = document.getElementById('recording-indicator');
        const interimTextElement = document.getElementById('interim-text');
        const finalTextElement = document.getElementById('final-text');
        const modelBadge = document.getElementById('model-badge');
        
        // Remove any old buttons that were added programmatically
        const oldButtons = document.querySelectorAll('main > button');
        oldButtons.forEach(button => {
            if (button.textContent === 'Start Speech Recognition' || 
                button.textContent === 'Load Speech Recognition Model') {
                button.remove();
            }
        });
        
        // Load model button setup
        loadModelBtn.addEventListener('click', async () => {
            loadModelBtn.disabled = true;
            loadModelBtn.textContent = 'Loading Model...';
            try {
                await initializeVosk();
                loadModelBtn.textContent = 'Model Loaded';
                loadModelBtn.style.backgroundColor = '#4CAF50'; // Green color for success
                modelLoaded = true;
                updateModelStatus('vosk');
            } catch (error) {
                console.error('Error loading Vosk model:', error);
                loadModelBtn.textContent = 'Load Model';
                loadModelBtn.disabled = false;
                updateModelStatus('none');
                alert('Failed to load the speech recognition model. Please check your connection.');
            }
        });

        // Initialize Vosk model
        async function initializeVosk() {
            try {
                // Create audio context for sample rate
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const sampleRate = audioContext.sampleRate;Content = 'Downloading model...';
                
                // Create and initialize Web Worker for Vosk processingInitialize Vosk with proper model loading
                if (voskWorker) {ccoreilly/vosk-browser@master/public/models/vosk-model-small-en-us-0.15';
                    voskWorker.terminate();ng Vosk model from:', modelUrl);
                }
                odel
                voskWorker = new Worker('js/vosk-worker.js');reateModel(modelUrl);
                   const model = await vosk.createModel(modelUrl);
                // Set up worker message handler       
                voskWorker.onmessage = function(event) {        recognizer = new vosk.Recognizer({
                    const message = event.data;
                    t.sampleRate
                    switch(message.status) {
                        case 'workerReady':    
                            console.log('Vosk worker is ready');('Vosk model loaded successfully with sample rate:', audioContext.sampleRate);
                            break;;
                            
                        case 'loading':r);
                            loadModelBtn.textContent = 'Downloading...';r;
                            break;
                            
                        case 'init':
                            loadModelBtn.textContent = 'Initializing...';atus indicator
                            break;odelStatus(model) {
                            
                        case 'ready':
                            console.log('Vosk model loaded successfully');witch(model) {
                            modelLoaded = true;       case 'webspeech':
                            break;            modelBadge.classList.add('web-speech');
                             'Web Speech API';
                        case 'error':
                            console.error('Vosk error:', message.message);
                            loadModelBtn.textContent = 'Load Model';
                            loadModelBtn.disabled = false;
                            throw new Error(message.message);
                            default:
                        case 'result':del');
                            if (message.type === 'partial') {
                                interimTextElement.textContent = message.text;
                            } else {
                                finalTextElement.textContent += message.text + ' ';
                                interimTextElement.textContent = '';
                            }() {
                            break;API if available (better for real-time results)
                    }'webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
                };peech API for speech recognition');
                );
                // Load the model in the worker
                voskWorker.postMessage({ Create speech recognition object
                    command: 'loadModel',const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                    sampleRate: sampleRate,echRecognition();
                    modelUrl: 'https://cdn.jsdelivr.net/gh/ccoreilly/vosk-browser@master/public/models/vosk-model-small-en-us-0.15'
                }); Configure recognition
                recognition.continuous = true;
                // Wait for model to be ready
                return new Promise((resolve, reject) => {
                    const timeout = setTimeout(() => {
                        reject(new Error('Model loading timeout after 30 seconds'));et up event handlers
                    }, 30000);() => {
                    
                    voskWorker.addEventListener('message', function handler(event) {
                        if (event.data.status === 'ready') {
                            clearTimeout(timeout);d = () => {
                            voskWorker.removeEventListener('message', handler);
                            resolve(true);
                        } else if (event.data.status === 'error') {
                            clearTimeout(timeout);gnition.onresult = (event) => {
                            voskWorker.removeEventListener('message', handler);
                            reject(new Error(event.data.message));;
                        }
                    });/ Process results
                });for (let i = event.resultIndex; i < event.results.length; ++i) {
            } catch (error) {i].isFinal) {
                console.error('Error initializing Vosk:', error);].transcript + ' ';
                throw error;
            }[0].transcript;
        }
        
        // Update model status indicator  
        function updateModelStatus(model) {    // Update the UI with results
            modelBadge.className = 'model-badge';
            
            switch(model) {
                case 'webspeech':
                    modelBadge.classList.add('web-speech');  if (finalTranscript) {
                    modelBadge.textContent = 'Web Speech API';        // Append to final text, don't replace it
                    break;tent += finalTranscript;
                case 'vosk':d
                    modelBadge.classList.add('vosk-model');xtContent = '';
                    modelBadge.textContent = 'Vosk Model';
                    break;
                default:
                    modelBadge.classList.add('no-model');
                    modelBadge.textContent = 'Not Selected';le.error('Speech recognition error:', event.error);
            }etRecordingState(false);
        } alert(`Speech recognition error: ${event.error}`);
        };
        // Speech recognition initialization
        async function initializeSpeechRecognition() {
            // First try to use Web Speech API if available (better for real-time results)ner('click', () => {
            if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) { {
                console.log('Using Web Speech API for speech recognition');   try {
                updateModelStatus('webspeech');         recognition.start();
                        } catch (error) {
                // Create speech recognition object        console.error('Failed to start recognition:', error);
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                const recognition = new SpeechRecognition();
                
                // Configure recognition
                recognition.continuous = true;on
                recognition.interimResults = true;
                recognition.lang = 'en-US';ive) {
                
                // Set up event handlers
                recognition.onstart = () => {
                    setRecordingState(true);
                };{
                // Fall back to Vosk for browsers without Web Speech API or offline use
                recognition.onend = () => {not available, using Vosk');
                    setRecordingState(false);d
                };   
                       // Set up start speech button
                recognition.onresult = (event) => {        startSpeechBtn.addEventListener('click', () => {
                    let interimTranscript = '';ed) {
                    let finalTranscript = '';
                    
                    // Process results            alert('Please load the speech model first by clicking the "Load Speech Model" button.');
                    for (let i = event.resultIndex; i < event.results.length; ++i) {   }
                        if (event.results[i].isFinal) {
                            finalTranscript += event.results[i][0].transcript + ' ';
                        } else {
                            interimTranscript += event.results[i][0].transcript;stopSpeechBtn.addEventListener('click', stopVoskRecognition);
                        }
                    }
                    
                    // Update the UI with results
                    if (interimTranscript) {
                        interimTextElement.textContent = interimTranscript;
                    }
                    
                    if (finalTranscript) {tUserMedia({ audio: true });
                        // Append to final text, don't replace ite = audioContext.createMediaStreamSource(audioStream);
                        finalTextElement.textContent += finalTranscript;essor = audioContext.createScriptProcessor(4096, 1, 1);
                        // Clear interim text once it's finalized
                        interimTextElement.textContent = '';
                    }essor.connect(audioContext.destination);
                };
                s = (event) => {
                recognition.onerror = (event) => {
                    console.error('Speech recognition error:', event.error);ent.inputBuffer.getChannelData(0);
                    setRecordingState(false);
                    alert(`Speech recognition error: ${event.error}`);let i = 0; i < inputData.length; i++) {
                };6Array[i] = inputData[i] * 32767;
                
                // Function to start recognition
                startSpeechBtn.addEventListener('click', () => {
                    if (!recognitionActive) {
                        try {
                            recognition.start();f (result) {
                        } catch (error) {      // Final result
                            console.error('Failed to start recognition:', error);        const finalText = recognizer.result().text;
                        }
                    }
                });
                
                // Function to stop recognition
                stopSpeechBtn.addEventListener('click', () => { = recognizer.partialResult().partial;
                    if (recognitionActive) {           if (partialText) {
                        recognition.stop();                   interimTextElement.textContent = partialText;
                    }                }
                });
                
            } else {
                // Fall back to Vosk for browsers without Web Speech API or offline use    setRecordingState(true);
                console.log('Web Speech API not available, using Vosk');('vosk'); // Ensure model status is updated when using Vosk
                updateModelStatus('none'); // Start with no model until loadedr) {
                 starting Vosk recognition:', error);
                // Set up start speech buttonto access the microphone. Please ensure you have granted permission.');
                startSpeechBtn.addEventListener('click', () => {   setRecordingState(false);
                    if (modelLoaded) {}
                        startVoskRecognition();
                    } else {
                        alert('Please load the speech model first by clicking the "Load Speech Model" button.');
                    }ion stopVoskRecognition() {
                });if (!recognitionActive) return;
                
                // Set up stop speech button
                stopSpeechBtn.addEventListener('click', stopVoskRecognition);
            }   source.disconnect();
        }    source = null;
        
        // Start Vosk recognition
        async function startVoskRecognition() {   if (processor) {
            if (recognitionActive || !voskWorker) return;        processor.disconnect();
            
            try {
                audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                audioContext = new (window.AudioContext || window.webkitAudioContext)();if (audioStream) {
                source = audioContext.createMediaStreamSource(audioStream);ks().forEach(track => track.stop());
                
                // Use ScriptProcessorNode (will be replaced with AudioWorkletNode in future version)
                processor = audioContext.createScriptProcessor(4096, 1, 1);
                
                source.connect(processor);false);
                processor.connect(audioContext.destination);
                
                // Reset the recognizer statenction to update UI for recording state
                voskWorker.postMessage({ command: 'reset' });
                
                processor.onaudioprocess = (event) => {
                    // Only process if recognition is active// Update button states
                    if (recognitionActive) {ding;
                        // Convert audio data for Voskled = !isRecording;
                        const inputData = event.inputBuffer.getChannelData(0);
                        const int16Array = new Int16Array(inputData.length);/ Update recording indicator
                        for (let i = 0; i < inputData.length; i++) {   if (isRecording) {
                            int16Array[i] = Math.min(1, Math.max(-1, inputData[i])) * 32767;        recordingIndicator.className = 'recording-on';
                        }ecording';
                        
                        // Send to worker for processing   recordingIndicator.className = 'recording-off';
                        voskWorker.postMessage({                recordingIndicator.textContent = '● Recording';
                            command: 'processAudio',}
                            audio: int16Array
                        });m text if stopping
                    }
                };
                
                setRecordingState(true);
                
            } catch (error) {// Initialize speech recognition on page load
                console.error('Error starting Vosk recognition:', error);OMContentLoaded', initializeSpeechRecognition);
                alert('Failed to access the microphone. Please ensure you have granted permission.');
                setRecordingState(false);
            }
        }
        
        // Stop Vosk recognition
        function stopVoskRecognition() {
            if (!recognitionActive) return;onst lastUpdatedSpan = document.getElementById('last-updated');
            const dynamicContent = document.getElementById('dynamic-content');
            // Clean up resourcesefresh-button');
            if (source) {
                source.disconnect();
                source = null;().toLocaleString();
            }
            and update the UI
            if (processor) {
                processor.disconnect();
                processor = null;ou are offline. Using cached content.';
            }isOnline ? 'status online' : 'status offline';
            
            if (audioContext && audioContext.state !== 'closed') {
                audioContext.suspend();online/offline events
            }
            Listener('offline', updateOnlineStatus);
            if (audioStream) {nlineStatus(); // Initial check
                audioStream.getTracks().forEach(track => track.stop());
                audioStream = null;// Service Worker Registration
            }igator) {
            
            // Reset UI
            setRecordingState(false);> {
        }red!', reg);
        
        // Helper function to update UI for recording state
        function setRecordingState(isRecording) {r registration failed:', err);
            recognitionActive = isRecording;         });
                });
            // Update button states
            startSpeechBtn.disabled = isRecording;
            stopSpeechBtn.disabled = !isRecording;
            nstallprompt', (e) => {
            // Update recording indicatorly showing the prompt
            if (isRecording) {
                recordingIndicator.className = 'recording-on';
                recordingIndicator.textContent = '● Recording';
            } else {
                recordingIndicator.className = 'recording-off';yle.display = 'block';
                recordingIndicator.textContent = '● Recording';
            }
            ick', () => {
            // Clear interim text if stoppingerredPrompt) {
            if (!isRecording) {   // Show the install prompt
                interimTextElement.textContent = '';     deferredPrompt.prompt();
            }        // Wait for the user to respond to the prompt
        }en((choiceResult) => {
        Result.outcome === 'accepted') {
        // Initialize speech recognition on page loadstall prompt');
        document.addEventListener('DOMContentLoaded', initializeSpeechRecognition);y = 'none';
    </script> {
console.log('User dismissed the install prompt');
    <script>
        // Initialize variables
        let deferredPrompt;
        const installButton = document.getElementById('install-button');
        const statusDiv = document.getElementById('connection-status');
        const lastUpdatedSpan = document.getElementById('last-updated');
        const dynamicContent = document.getElementById('dynamic-content');imulate fetching dynamic content
        const refreshButton = document.getElementById('refresh-button');
        etch from an API
        // Set the last updated time with local data
        lastUpdatedSpan.textContent = new Date().toLocaleString();
        
        // Check if the app is online and update the UI title: 'Item 1', description: 'Description for item 1' },
        function updateOnlineStatus() {n: 'Description for item 2' },
            const isOnline = navigator.onLine;e: 'Item 3', description: 'Description for item 3' }
            statusDiv.textContent = isOnline ? 'You are online.' : 'You are offline. Using cached content.';
            statusDiv.className = isOnline ? 'status online' : 'status offline';
        }
        
        // Add event listeners for online/offline eventsromise((resolve) => {
        window.addEventListener('online', updateOnlineStatus); // Simulate network delay
        window.addEventListener('offline', updateOnlineStatus);       setTimeout(() => {
        updateOnlineStatus(); // Initial check            if (navigator.onLine) {
        
        // Service Worker Registration
        if ('serviceWorker' in navigator) {
            window.addEventListener('load', () => {            resolve({
                navigator.serviceWorker.register('service-worker.js'): 0, title: 'Cached Item', description: 'This is cached content for offline use' }],
                    .then(reg => {imestamp: 'Using cached data'
                        console.log('Service worker registered!', reg);
                    })
                    .catch(err => {;
                        console.log('Service worker registration failed:', err);
                    });
            });
        }
        unction updateUI(data) {
        // Handle app installation    let html = `<p>Last fetched: ${data.timestamp}</p><ul>`;
        window.addEventListener('beforeinstallprompt', (e) => {
            // Prevent Chrome 67 and earlier from automatically showing the prompt => {
            e.preventDefault();        html += `<li>
            // Stash the event so it can be triggered latertle}</h3>
            deferredPrompt = e;
            // Show the install button
            installButton.style.display = 'block';
        }); 
        tml += '</ul>';
        installButton.addEventListener('click', () => {     dynamicContent.innerHTML = html;
            if (deferredPrompt) { }
                // Show the install prompt        






































































</html></body>    </script>        });            fetchData().then(updateUI);            dynamicContent.innerHTML = '<p>Loading...</p>';        refreshButton.addEventListener('click', () => {        // Refresh button handler                fetchData().then(updateUI);        // Initial data fetch                }            dynamicContent.innerHTML = html;            html += '</ul>';                        });                </li>`;                    <p>${item.description}</p>                    <h3>${item.title}</h3>                html += `<li>            data.items.forEach(item => {                        let html = `<p>Last fetched: ${data.timestamp}</p><ul>`;        function updateUI(data) {        // Update the UI with fetched data                }            });                }, 500);                    }                        });                            timestamp: 'Using cached data'                            items: [{ id: 0, title: 'Cached Item', description: 'This is cached content for offline use' }],                        resolve({                        // Return cached data if offline                    } else {                        resolve(data);                    if (navigator.onLine) {                setTimeout(() => {                // Simulate network delay            return new Promise((resolve) => {                        };                timestamp: new Date().toLocaleString()                ],                    { id: 3, title: 'Item 3', description: 'Description for item 3' }                    { id: 2, title: 'Item 2', description: 'Description for item 2' },                    { id: 1, title: 'Item 1', description: 'Description for item 1' },                items: [            const data = {            // Here we'll simulate with local data            // In a real app, you would fetch from an API        function fetchData() {        // Simulate fetching dynamic content                });            }                });                    deferredPrompt = null;                    }                        console.log('User dismissed the install prompt');                    } else {                        installButton.style.display = 'none';                        console.log('User accepted the install prompt');                    if (choiceResult.outcome === 'accepted') {                deferredPrompt.userChoice.then((choiceResult) => {                // Wait for the user to respond to the prompt                deferredPrompt.prompt();        // Initial data fetch
        fetchData().then(updateUI);
        
        // Refresh button handler
        refreshButton.addEventListener('click', () => {
            dynamicContent.innerHTML = '<p>Loading...</p>';
            fetchData().then(updateUI);
        });
    </script>
</body>
</html>
